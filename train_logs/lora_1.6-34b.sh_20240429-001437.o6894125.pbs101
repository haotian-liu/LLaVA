[2024-04-29 00:15:06,117] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-04-29 00:16:32,465] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-04-29 00:16:32,466] [INFO] [runner.py:568:main] cmd = /home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path /home/users/ntu/chih0001/scratch/model/llava-v1.6-34b --version v1 --data_path /home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json --image_folder ./playground/data --vision_tower /home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /home/users/ntu/chih0001/scratch/model/lora/globalBS/llava-v1.6-34b-DSLLM-lora --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2024-04-29 00:16:37,095] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-04-29 00:16:39,253] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-04-29 00:16:39,253] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-29 00:16:39,253] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-29 00:16:39,253] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-29 00:16:39,253] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-04-29 00:16:39,254] [INFO] [launch.py:253:main] process 1500462 spawned with command: ['/home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-34b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/globalBS/llava-v1.6-34b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2024-04-29 00:16:39,255] [INFO] [launch.py:253:main] process 1500463 spawned with command: ['/home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-34b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/globalBS/llava-v1.6-34b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2024-04-29 00:16:39,256] [INFO] [launch.py:253:main] process 1500464 spawned with command: ['/home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-34b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/globalBS/llava-v1.6-34b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2024-04-29 00:16:39,256] [INFO] [launch.py:253:main] process 1500465 spawned with command: ['/home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-34b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/globalBS/llava-v1.6-34b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2024-04-29 00:17:02,770] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-29 00:17:02,770] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-29 00:17:02,770] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-29 00:17:02,771] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.

[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-04-29 00:17:05,225] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-29 00:17:05,225] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-29 00:17:05,226] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-29 00:17:05,226] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-29 00:17:05,226] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2024-04-29 00:17:25,620] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 939, num_elems = 34.75B
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:00<00:07,  1.92it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:00<00:07,  1.96it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:00<00:07,  1.90it/s]Loading checkpoint shards:  13%|█▎        | 2/15 [00:01<00:06,  1.94it/s]Loading checkpoint shards:  13%|█▎        | 2/15 [00:01<00:06,  1.91it/s]Loading checkpoint shards:  13%|█▎        | 2/15 [00:01<00:06,  1.92it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:05<01:12,  5.21s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:10<01:10,  5.44s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:14<01:19,  6.62s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:14<01:19,  6.64s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:14<01:19,  6.64s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:15<00:46,  4.23s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:15<00:46,  4.23s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:15<00:46,  4.23s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:16<00:29,  2.95s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:16<00:29,  2.95s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:16<00:29,  2.95s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:16<01:05,  5.42s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:21<00:58,  5.32s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:26<00:53,  5.32s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:29<00:59,  6.58s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:29<00:59,  6.58s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:29<00:59,  6.59s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:30<00:37,  4.66s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:30<00:37,  4.67s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:30<00:37,  4.67s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:31<00:23,  3.38s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:31<00:23,  3.38s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:31<00:23,  3.39s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:32<00:48,  5.34s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:37<00:42,  5.32s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:42<00:35,  5.14s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:44<00:38,  6.48s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:44<00:38,  6.48s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:44<00:38,  6.49s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:45<00:23,  4.74s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:45<00:23,  4.74s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:45<00:23,  4.74s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:45<00:13,  3.46s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:45<00:13,  3.46s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:45<00:13,  3.46s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:47<00:30,  5.12s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:52<00:25,  5.15s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:57<00:20,  5.07s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:58<00:18,  6.25s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:58<00:18,  6.26s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:58<00:18,  6.26s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:59<00:09,  4.54s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:59<00:09,  4.54s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:59<00:09,  4.54s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:02<00:15,  5.02s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:07<00:10,  5.09s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:11<00:06,  6.82s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:11<00:06,  6.82s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:11<00:06,  6.82s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [01:12<00:04,  4.96s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:14<00:00,  5.73s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:14<00:00,  4.96s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [01:14<00:00,  5.73s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:14<00:00,  4.96s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [01:14<00:00,  5.73s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:14<00:00,  4.96s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [01:15<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 15/15 [01:15<00:00,  5.03s/it]
Adding LoRA adapters...
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Formatting inputs...Skip in lazy mode
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Parameter Offload: Total persistent parameters: 1213440 in 369 params
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)WARNING: tokenization mismatch: 79 vs. 81. (ignored)WARNING: tokenization mismatch: 78 vs. 80. (ignored)


WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)WARNING: tokenization mismatch: 142 vs. 144. (ignored)

WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
wandb: Currently logged in as: haozhuangchi. Use `wandb login --relogin` to force relogin
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)WARNING: tokenization mismatch: 141 vs. 143. (ignored)

WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)WARNING: tokenization mismatch: 79 vs. 81. (ignored)

WARNING: tokenization mismatch: 79 vs. 81. (ignored)WARNING: tokenization mismatch: 141 vs. 143. (ignored)

WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)WARNING: tokenization mismatch: 78 vs. 80. (ignored)

WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
wandb: - Waiting for wandb.init()...WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
wandb: \ Waiting for wandb.init()...WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /scratch/users/ntu/chih0001/VLM/LLaVA/wandb/run-20240429_001933-m4gh0tks
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-glitter-26
wandb: ⭐️ View project at https://wandb.ai/haozhuangchi/huggingface
wandb: 🚀 View run at https://wandb.ai/haozhuangchi/huggingface/runs/m4gh0tks
  0%|          | 0/658 [00:00<?, ?it/s]WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)WARNING: tokenization mismatch: 141 vs. 143. (ignored)

WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)WARNING: tokenization mismatch: 142 vs. 144. (ignored)

WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)WARNING: tokenization mismatch: 142 vs. 144. (ignored)

WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
Traceback (most recent call last):
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train.py", line 969, in train
    trainer.train()
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 2772, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 2795, in compute_loss
    outputs = model(**inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/peft_model.py", line 1129, in forward
    return self.base_model(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 161, in forward
    return self.model.forward(*args, **kwargs)
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/model/language_model/llava_llama.py", line 91, in forward
    return super().forward(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1060, in forward
    layer_outputs = self._gradient_checkpointing_func(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 451, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 230, in forward
    outputs = run_function(*args)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 812, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 268, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/tuners/lora/layer.py", line 509, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 900.00 MiB. GPU 2 has a total capacty of 39.39 GiB of which 721.12 MiB is free. Including non-PyTorch memory, this process has 38.68 GiB memory in use. Of the allocated memory 31.77 GiB is allocated by PyTorch, and 5.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-04-29 00:20:07,535] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1500462
[2024-04-29 00:20:07,912] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1500463
[2024-04-29 00:20:08,244] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1500464
[2024-04-29 00:20:08,244] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1500465
[2024-04-29 00:20:08,643] [ERROR] [launch.py:322:sigkill_handler] ['/home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-34b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/globalBS/llava-v1.6-34b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1
[2024-04-29 00:20:14,992] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-04-29 00:20:17,190] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3: setting --include=localhost:0,1,2,3
[2024-04-29 00:20:17,190] [INFO] [runner.py:568:main] cmd = /home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path /home/users/ntu/chih0001/scratch/model/llava-v1.6-34b --version v1 --data_path /home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json --image_folder ./playground/data --vision_tower /home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /home/users/ntu/chih0001/scratch/model/lora/llava-v1.6-34b-DSLLM-lora --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2024-04-29 00:20:21,709] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-04-29 00:20:23,865] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-04-29 00:20:23,865] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-29 00:20:23,865] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-29 00:20:23,865] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-29 00:20:23,865] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-04-29 00:20:23,866] [INFO] [launch.py:253:main] process 1504444 spawned with command: ['/home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-34b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/llava-v1.6-34b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2024-04-29 00:20:23,866] [INFO] [launch.py:253:main] process 1504445 spawned with command: ['/home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-34b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/llava-v1.6-34b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2024-04-29 00:20:23,867] [INFO] [launch.py:253:main] process 1504446 spawned with command: ['/home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-34b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/llava-v1.6-34b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2024-04-29 00:20:23,868] [INFO] [launch.py:253:main] process 1504447 spawned with command: ['/home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-34b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/llava-v1.6-34b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb']
[2024-04-29 00:20:33,696] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-29 00:20:33,696] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-29 00:20:33,696] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-29 00:20:33,705] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.

[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-04-29 00:20:35,066] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-29 00:20:35,066] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-29 00:20:35,066] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-29 00:20:35,066] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-29 00:20:35,067] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2024-04-29 00:20:50,673] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 939, num_elems = 34.75B
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:00<00:04,  3.21it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:00<00:04,  3.17it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:00<00:04,  3.10it/s]Loading checkpoint shards:  13%|█▎        | 2/15 [00:00<00:02,  5.17it/s]Loading checkpoint shards:  13%|█▎        | 2/15 [00:00<00:02,  5.09it/s]Loading checkpoint shards:  13%|█▎        | 2/15 [00:00<00:02,  4.99it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:01<00:24,  1.72s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:03<00:19,  1.54s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:04<00:21,  1.75s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:04<00:21,  1.76s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:04<00:21,  1.77s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:04<00:12,  1.13s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:04<00:12,  1.13s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:04<00:12,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:04<00:07,  1.27it/s]Loading checkpoint shards:  33%|███▎      | 5/15 [00:04<00:07,  1.26it/s]Loading checkpoint shards:  33%|███▎      | 5/15 [00:04<00:07,  1.25it/s]Loading checkpoint shards:  20%|██        | 3/15 [00:04<00:18,  1.54s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:06<00:16,  1.47s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:07<00:14,  1.43s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:08<00:15,  1.76s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:08<00:15,  1.77s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:08<00:16,  1.79s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:08<00:09,  1.25s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:08<00:09,  1.25s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:08<00:10,  1.26s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:08<00:06,  1.10it/s]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:08<00:06,  1.10it/s]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:08<00:06,  1.08it/s]Loading checkpoint shards:  40%|████      | 6/15 [00:08<00:13,  1.47s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:10<00:11,  1.46s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:11<00:09,  1.43s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:12<00:10,  1.79s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:12<00:10,  1.82s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:12<00:06,  1.30s/it]Loading checkpoint shards:  60%|██████    | 9/15 [00:12<00:11,  1.84s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:12<00:06,  1.31s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:12<00:06,  1.32s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:12<00:03,  1.03it/s]Loading checkpoint shards:  60%|██████    | 9/15 [00:13<00:08,  1.48s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:14<00:07,  1.46s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:15<00:07,  1.98s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:15<00:07,  1.99s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:16<00:04,  1.44s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:16<00:04,  1.46s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:16<00:05,  1.46s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:16<00:02,  1.07s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:16<00:02,  1.08s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:16<00:05,  1.86s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:16<00:02,  1.35s/it]Loading checkpoint shards:  80%|████████  | 12/15 [00:17<00:04,  1.48s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:19<00:02,  1.46s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:19<00:01,  1.83s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:19<00:01,  1.84s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:20<00:02,  2.02s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:20<00:01,  1.46s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:21<00:00,  1.66s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:21<00:00,  1.41s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [00:21<00:00,  1.66s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:21<00:00,  1.41s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [00:21<00:00,  1.64s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:21<00:00,  1.41s/it]
Loading checkpoint shards: 100%|██████████| 15/15 [00:21<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 15/15 [00:21<00:00,  1.43s/it]
Adding LoRA adapters...
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
openai/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Formatting inputs...Skip in lazy mode
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Parameter Offload: Total persistent parameters: 1213440 in 369 params
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)WARNING: tokenization mismatch: 141 vs. 143. (ignored)

WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 77 vs. 79. (ignored)WARNING: tokenization mismatch: 78 vs. 80. (ignored)

WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 143 vs. 145. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
wandb: Currently logged in as: haozhuangchi. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /scratch/users/ntu/chih0001/VLM/LLaVA/wandb/run-20240429_002200-lr05h0ss
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-firebrand-27
wandb: ⭐️ View project at https://wandb.ai/haozhuangchi/huggingface
wandb: 🚀 View run at https://wandb.ai/haozhuangchi/huggingface/runs/lr05h0ss
  0%|          | 0/1316 [00:00<?, ?it/s]WARNING: tokenization mismatch: 146 vs. 148. (ignored)WARNING: tokenization mismatch: 146 vs. 148. (ignored)WARNING: tokenization mismatch: 146 vs. 148. (ignored)


WARNING: tokenization mismatch: 142 vs. 144. (ignored)WARNING: tokenization mismatch: 141 vs. 143. (ignored)

WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 146 vs. 148. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 142 vs. 144. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 141 vs. 143. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 79 vs. 81. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
WARNING: tokenization mismatch: 78 vs. 80. (ignored)
Traceback (most recent call last):
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train.py", line 969, in train
    trainer.train()
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
Traceback (most recent call last):
      File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train_mem.py", line 4, in <module>
return inner_training_loop(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
    train(attn_implementation="flash_attention_2")
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train.py", line 969, in train
    trainer.train()
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 2772, in training_step
    return inner_training_loop(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 2772, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 2795, in compute_loss
    loss = self.compute_loss(model, inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 2795, in compute_loss
    outputs = model(**inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    outputs = model(**inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
Traceback (most recent call last):
    return forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train.py", line 969, in train
    trainer.train()
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 2772, in training_step
    loss = self.compute_loss(model, inputs)
    loss = self.module(*inputs, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 2795, in compute_loss
    outputs = model(**inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
    return forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
    loss = self.module(*inputs, **kwargs)
    ret_val = func(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/peft_model.py", line 1129, in forward
    return self.base_model(
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 161, in forward
    return self.model.forward(*args, **kwargs)
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/model/language_model/llava_llama.py", line 91, in forward
    return super().forward(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1202, in forward
    logits = logits.float()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.87 GiB is free. Including non-PyTorch memory, this process has 37.51 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 6.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    loss = self.module(*inputs, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/peft_model.py", line 1129, in forward
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    return self.base_model(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/peft_model.py", line 1129, in forward
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    return self.base_model(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 161, in forward
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    return self.model.forward(*args, **kwargs)
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/model/language_model/llava_llama.py", line 91, in forward
    return super().forward(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1202, in forward
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 161, in forward
Traceback (most recent call last):
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation="flash_attention_2")
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/train/train.py", line 969, in train
    logits = logits.float()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.74 GiB. GPU 3 has a total capacty of 39.39 GiB of which 2.34 GiB is free. Including non-PyTorch memory, this process has 37.04 GiB memory in use. Of the allocated memory 30.03 GiB is allocated by PyTorch, and 5.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return self.model.forward(*args, **kwargs)
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/model/language_model/llava_llama.py", line 91, in forward
    trainer.train()
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return super().forward(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1202, in forward
    logits = logits.float()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.74 GiB. GPU 1 has a total capacty of 39.39 GiB of which 2.22 GiB is free. Including non-PyTorch memory, this process has 37.17 GiB memory in use. Of the allocated memory 30.03 GiB is allocated by PyTorch, and 5.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return inner_training_loop(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 2772, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/trainer.py", line 2795, in compute_loss
    outputs = model(**inputs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/peft_model.py", line 1129, in forward
    return self.base_model(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1568, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 161, in forward
    return self.model.forward(*args, **kwargs)
  File "/scratch/users/ntu/chih0001/VLM/LLaVA/llava/model/language_model/llava_llama.py", line 91, in forward
    return super().forward(
  File "/home/users/ntu/chih0001/anaconda3/envs/llava-test/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1202, in forward
    logits = logits.float()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.74 GiB. GPU 2 has a total capacty of 39.39 GiB of which 2.09 GiB is free. Including non-PyTorch memory, this process has 37.29 GiB memory in use. Of the allocated memory 30.01 GiB is allocated by PyTorch, and 5.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: - 0.017 MB of 0.017 MB uploaded[2024-04-29 00:22:14,023] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1504444
[2024-04-29 00:22:14,429] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1504445
[2024-04-29 00:22:14,430] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1504446
[2024-04-29 00:22:14,449] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1504447
[2024-04-29 00:22:14,466] [ERROR] [launch.py:322:sigkill_handler] ['/home/users/ntu/chih0001/anaconda3/envs/llava-test/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', './scripts/zero3.json', '--model_name_or_path', '/home/users/ntu/chih0001/scratch/model/llava-v1.6-34b', '--version', 'v1', '--data_path', '/home/users/ntu/chih0001/scratch/VLM/LLaVA/train/lora.json', '--image_folder', './playground/data', '--vision_tower', '/home/users/ntu/chih0001/scratch/model/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/users/ntu/chih0001/scratch/model/lora/llava-v1.6-34b-DSLLM-lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb'] exits with return code = 1
